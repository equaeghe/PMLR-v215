@proceedings{ISIPTA-2023,
	booktitle = {Proceedings of the Thirteenth International Symposium on Imprecise Probability: Theories and Applications},
	name = {International Symposium on Imprecise Probability: Theories and Applications},
	shortname = {ISIPTA},
	editor = {Miranda, Enrique and Montes, Ignacio and Quaeghebeur, Erik and Vantaggi, Barbara},
	sections = {Preface|Papers},
	volume = {215},
	year = {2023},
	start = {2023-07-11},
	end = {2023-07-14},
	published = {2023-07-10},
	address = {Oviedo, Spain},
	conference_url = {https://isipta23.sipta.org/},
	conference_number = {13}
}

@inproceedings{miranda23a,
	title = {{ISIPTA} 2023: {P}reface},
	author = {Miranda, Enrique and Montes, Ignacio and Quaeghebeur, Erik and Vantaggi, Barbara},
	pages = {1--2},
	abstract = {The ISIPTA meetings are the primary international forum for presenting and discussing advances in the theory and applications of imprecise probabilities. They are organized once every two years by SIPTA, <em>The Society for Imprecise Probabilities: Theories and Applications</em>. The first edition took place in Ghent in 1999. The 12th edition of 2021, was essentially a virtual event due to the Covid-19 crisis. The year 2023 marks the return to a full in-person conference. The <em>13th International Conference on Imprecise Probabilities: Theories and Applications</em> will be held in Oviedo, from Tuesday 11 to Friday 14 July 2023.},
	keywords = {preface, committees, acknowledgements},
	section = {Preface}
}

@inproceedings{alonsodelafuente23a,
	title = {Sets of probability measures and convex combination spaces},
	author = {Alonso de la Fuente, Miriam and Ter\'an, Pedro},
	pages = {3--10},
	abstract = {The Wasserstein distances between probability distributions are an important tool in modern probability theory which has been generalized to sets of probability distributions. We will show that the (generalized) $L^1$-Wasserstein metric, with the operations of convolution and rescaling, fits in the abstract framework of convex combination spaces: nonlinear metric spaces preserving some of the nice properties of a normed space but accomodating other unusual behaviours. For instance, unlike in a linear space, a singleton $\{P\}$ is typically not convex (it is so only if $P$ is degenerate). Also, some theorems for convex combination spaces are applied to this setting.},
	keywords = {compact sets of probabilities, convolution, credal set, law of large numbers, Wasserstein metric},
	section = {Papers}
}

@inproceedings{bailie23a,
	title = {Differential privacy: general inferential limits via intervals of measures},
	author = {Bailie, James and Gong, Ruobin},
	pages = {11--24},
	abstract = {Differential privacy (DP) is a mathematical standard for assessing the privacy provided by a data-release mechanism. We provide formulations of pure $\epsilon$-differential privacy first as a Lipschitz continuity condition and then using an object from the imprecise probability literature: the interval of measures. We utilise this second formulation to establish bounds on the appropriate likelihood function for $\epsilon$-DP data -- and in turn derive limits on key quantities in both frequentist hypothesis testing and Bayesian inference. Under very mild conditions, these results are valid for arbitrary parameters, priors and data generating models. These bounds are weaker than those attainable when analysing specific data generating models or data-release mechanisms. However, they provide generally applicable limits on the ability to learn from differentially private data -- even when the analyst's knowledge of the model or mechanism is limited. They also shed light on the semantic interpretation of differential privacy, a subject of contention in the current literature.},
	keywords = {disclosure risk, prior-to-posterior semantics, Neyman-Pearson hypothesis testing, Lipschitz continuity, multiplicative distance, transparency, statistical disclosure control},
	section = {Papers}
}

@inproceedings{benavoli23a,
	title = {Closure operators, classifiers and desirability},
	author = {Benavoli, Alessio and Facchini, Alessandro and Zaffalon, Marco},
	pages = {25--36},
	abstract = {At the core of Bayesian probability theory, or dually desirability theory, lies an assumption of linearity of the scale in which rewards are measured. We revisit two recent papers that extend desirability theory to the nonlinear case by letting the utility scale be represented either by a general closure operator or by a binary general (nonlinear) classifier. By using standard results in logic, we highlight the connection between these two approaches and show that this connection allows us to extend the separating hyper plane theorem (which is at the core of the duality between Bayesian decision theory and desirability theory) to the nonlinear case.},
	keywords = {closure operators, classifiers, desirability, belief structure},
	section = {Papers}
}

@inproceedings{bilkova23a,
	title = {Describing and quantifying contradiction between pieces of evidence via {B}elnap {D}unn logic and {D}empster-{S}hafer theory},
	author = {B{\'\i}lkov\'a, Marta and Frittella, Sabine and Kozhemiachenko, Daniil and Majer, Ondrej and Manoorkar, Krishna},
	pages = {37--47},
	abstract = {Belnap Dunn logic is a four-valued logic introduced to model reasoning with incomplete or contradictory information. In this article, we show how Dempster-Shafer theory can be used over Belnap Dunn logic in order to formalise reasoning with incomplete and/or contradictory pieces of evidence. First, we discuss how to encode different kinds of evidence, and how to interpret the resulting belief and plausibility functions. Then, we discuss the behaviour of Dempster's rule in this framework and present a variation of the rule. Finally, we show how to construct credal sets of classical probability measures based on this kind of evidence.},
	keywords = {Dempster-Shafer theory, Belnap Dunn logic, contradictory evidence},
	section = {Papers}
}

@inproceedings{blackwell23a,
	title = {Representing suppositional decision theories with sets of desirable gambles},
	author = {Blackwell, Kevin},
	pages = {48--58},
	abstract = {The sets of desirable gambles framework has been well-studied as a tool for representing decision-making with imprecise probabilistic beliefs -- under the assumption of act-state independence. The question of this paper is: can we use sets of desirable gambles to represent decisions where the states do depend (e.g., causally or probabilistically) on the acts? In particular, I investigate two possible routes for representing suppositional decision theories with sets of desirable gambles, concluding that while one route works only for the subclass of SDTs representable by general imaging, the other route can represent any SDT whatsoever. After giving a fairly flat-footed representation, I investigate whether it's equivalent to a construction directly from the local (suppositional) desirability judgments; it isn't, but this latter construction represents a different aggregation rule applied to the same ``credal committee.'' Finally, I extend the representation to model uncertainty about the supposition rule itself, in addition to imprecise credences.},
	keywords = {suppositional decision theories, causal decision theory, evidential decision theory, sets of desirable gambles, imaging},
	section = {Papers}
}

@inproceedings{blocher23a,
	title = {Depth functions for partial orders with a descriptive analysis of machine learning algorithms},
	author = {Blocher, Hannah and Schollmeyer, Georg and Jansen, Christoph and Nalenz, Malte},
	pages = {59--71},
	abstract = {We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.},
	keywords = {partial orders, data depth, benchmarking, algorithm comparison, outlier detection, non-standard data},
	section = {Papers}
}

@inproceedings{caprio23a,
	title = {Dynamic precise and imprecise probability kinematics},
	author = {Caprio, Michele and Gong, Ruobin},
	pages = {72--83},
	abstract = {We introduce dynamic probability kinematics (DPK), a method for an agent to mechanically update subjective beliefs in the presence of partial information. We then generalize DPK to dynamic imprecise probability kinematics (DIPK), which allows the agent to express their initial beliefs via a set of probabilities in order to further take ambiguity into account. Examples are provided to illustrate how the methods work.},
	keywords = {subjective probability, Jeffrey's updating, imprecise probabilities, probability kinematics, Bayes' rule},
	section = {Papers}
}

@inproceedings{caprio23b,
	title = {Constriction for sets of probabilities},
	author = {Caprio, Michele and Seidenfeld, Teddy},
	pages = {84--95},
	abstract = {Given a set of probability measures $\mathcal{P}$ representing an agent's knowledge on the elements of a sigma-algebra $\mathcal{F}$, we can compute upper and lower bounds for the probability of any event $A\in\mathcal{F}$ of interest. A procedure generating a new assessment of beliefs is said to constrict $A$ if the bounds on the probability of $A$ after the procedure are contained in those before the procedure. It is well documented that (generalized) Bayes' updating does not allow for constriction, for all $A\in\mathcal{F}$. In this work, we show that constriction can take place with and without evidence being observed, and we characterize these possibilities.},
	keywords = {constriction, dilation, sets of probabilities, evidence, conditioning, forgetting},
	section = {Papers}
}

@inproceedings{castronovo23a,
	title = {A generalized notion of conjunction for two conditional events},
	author = {Castronovo, Lydia and Sanfilippo, Giuseppe},
	pages = {96--108},
	abstract = {Traditionally the conjunction of conditional events has been defined as a three-valued object. However, in this way classical logical and probabilistic properties are not preserved. In recent literature, a notion of conjunction of two conditional events as a suitable conditional random quantity, which satisfies classical probabilistic properties, has been deepened in the setting of coherence. In this framework the conjunction $(A|H) \wedge (B|K)$ of two conditional events $A|H$ and $B|K$ is defined as a five-valued object with set of possible values $\{1,0,x,y,z\}$, where $x=P(A|H), y=P(B|K)$, and $z=\mathbb{P}[(A|H) \wedge (B|K)]$. In this paper we propose a generalization of this object, denoted by $(A|H) \wedge_{a,b} (B|K)$, where the values $x$ and $y$ are replaced by two arbitrary values $a,b \in [0,1]$. Then, by means of a geometrical approach, we compute the set of all coherent assessments on the family $\{A|H,B|K,(A|H) \wedge_{a,b} (B|K)\}$, by also showing that in the general case the Fr\'echet-Hoeffding bounds for the conjunction are not satisfied. We also analyze some particular cases. Finally, we study coherence in the imprecise case of an interval-valued probability assessment and we consider further aspects on $(A|H) \wedge_{a,b} (B|K)$.},
	keywords = {coherence, conjunction, conditional events, conditional random quantity, prevision, imprecise probability, Fr\'echet-Hoeffding bounds, quasi conjunction},
	section = {Papers}
}

@inproceedings{cella23a,
	title = {Finite sample valid probabilistic inference on quantile regression},
	author = {Cella, Leonardo},
	pages = {109--118},
	abstract = {In most applications, uncertainty quantification in quantile regression take the form of set estimates for the regression coefficients. However, often a more informative type of uncertainty quantification is desired where other inference-related tasks can be performed, such as the assignment of (imprecise) probabilities to assertions of interest about (any feature of) the regression coefficients. Validity of these probabilities, in the sense that their values are well-calibrated in a frequentist sense, is fundamental to the trustworthiness of the drawn conclusions. This paper presents a nonparametric Inferential Model (IM) construction that offers provably valid probabilistic uncertainty quantification in quantile regression, even in finite sample settings. It is also shown that this IM can be used to derive finite sample confidence regions for (any feature of) the regression coefficients. As a result, regardless of the type of uncertainty quantification desired, the proposed IM offers an appealing solution to quantile regression problems.},
	keywords = {quantile regression, inferential models, possibility measure, nonparametric, finite sample, validity, model-free, confidence region},
	section = {Papers}
}

@inproceedings{corsi23a,
	title = {A modal logic for uncertainty: a completeness theorem},
	author = {Corsi, Esther Anna and Flaminio, Tommaso and Godo, Llu{\'\i}s and Hosni, Hykel},
	pages = {119--129},
	abstract = {In the present paper, we axiomatize a logic that allows a general approach for reasoning about probability functions, belief functions, lower probabilities and their corresponding duals. The formal setting we consider arises from combining a modal S5 necessity operator $\Box$ that applies to the formulas of the infinite-valued {\L}ukasiewicz logic with the unary modality $P$ that describes the behaviour of probability functions. The modality $P$ together with an S5 {modality} $\Box$ provides a language rich enough to characterise probability, belief and lower probability theories. For this logic, we provide an axiomatization and we prove that, once we restrict to suitable sublanguages, it turns out to be sound and complete with respect to belief functions and lower probability models.},
	keywords = {fuzzy logic, Dempster-Shafer belief functions, probability functions, imprecise probabilities, modal logic},
	section = {Papers}
}

@inproceedings{cozman23a,
	title = {Markov conditions and factorization in logical credal networks},
	author = {Cozman, Fabio Gagliardi},
	pages = {130--140},
	abstract = {We examine the recently proposed language of <em>Logical Credal Networks</em>, in particular investigating the consequences of various Markov conditions. We introduce the notion of structure for a Logical Credal Network and show that a structure without directed cycles leads to a well-known factorization result. For networks with directed cycles, we analyze the differences between Markov conditions, factorization results, and specification requirements.},
	keywords = {logical credal networks, probabilistic logic, Markov condition, factorization},
	section = {Papers}
}

@inproceedings{debock23a,
	title = {A theory of desirable things},
	author = {De Bock, Jasper},
	pages = {141--152},
	abstract = {Inspired by the theory of desirable gambles that is used to model uncertainty in the field of imprecise probabilities, I present a theory of desirable things. Its aim is to model a subject's beliefs about which things are desirable. What the things are is not important, nor is what it means for them to be desirable. It can be applied to gambles, calling them desirable if a subject accepts them, but also to pizzas, calling them desirable if my friend Arthur likes to eat them. Regardless of the particular things that are considered, inference rules are imposed by means of an abstract closure operator, and models that adhere to these rules are called coherent. I consider two types of models, each of which can capture a subject's beliefs about which things are desirable: sets of desirable things and sets of desirable sets of things. A crucial result is that the latter type can be represented by a set of the former.},
	keywords = {desirable things, coherence, finite coherence, representation theorem, closure operator},
	section = {Papers}
}

@inproceedings{decooman23a,
	title = {Desirable sets of things and their logic},
	author = {de Cooman, Gert and Van Camp, Arthur and De Bock, Jasper},
	pages = {153--164},
	abstract = {We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise coherent sets of desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.},
	keywords = {desirability, desirable sets of things, conservative inference, propositional logic, filter, prime filter, principal filter, representation},
	section = {Papers}
}

@inproceedings{derr23a,
	title = {The set structure of precision},
	author = {Derr, Rabanus and Williamson, Robert C.},
	pages = {165--176},
	abstract = {In literature on imprecise probability little attention is paid to the fact that imprecise probabilities are precise on some events. We show that this <em>system of precision</em> forms, under mild assumptions, a so-called (pre-)Dynkin-system. Interestingly, there are several settings, ranging from machine learning on partial data over frequential probability theory to quantum probability theory and decision making under uncertainty, in which a priori the probabilities are only desired to be precise on a specific underlying set system. Here, (pre-)Dynkin-systems have been adopted as systems of precision, too. Under extendability conditions those pre-Dynkin-systems equipped with probabilities can be embedded into algebras of sets. Surprisingly, the extendability conditions elaborated in a strand of work in quantum physics are equivalent to coherence. Thus, we link the literature on probabilities on pre-Dynkin-systems to the literature on imprecise probability. In fact, the system of precision and imprecise probabilities live in structural duality.},
	keywords = {pre-Dynkin-system, Dynkin-system, coherence, extendability, quantum probability, intersectability},
	section = {Papers}
}

@inproceedings{devos23a,
	title = {Indistinguishability through exchangeability in quantum mechanics?},
	author = {De Vos, Keano and de Cooman, Gert and De Bock, Jasper},
	pages = {177--188},
	abstract = {Arguments in quantum mechanics often involve systems of indistinguishable particles, such as electrons or photons. On the standard approach, the symmetrisation postulate is needed to model indistinguishable particles, and results in a theory of fermions and bosons. We investigate how indistinguishability can be implemented by incorporating structural assessments of symmetry in the sets of desirable measurements approach to uncertainty modelling in quantum mechanics, which is based on the theory of imprecise probabilities, and in particular on sets of desirable gambles. We show that an exchangeability assessment allows us to partially retrieve the concepts of fermions and bosons, but that in order to recover the complete fermion and boson framework, we need to rely on stronger symmetry assessments. We also lay bare the relationship between these stronger assessments and the count vector representation for sets of desirable measurements, which we argue corresponds to the commonly used second quantisation in quantum mechanics.},
	keywords = {exchangeability, quantum mechanics, indistinguishability, desirable measurements, strong symmetry, second quantisation},
	section = {Papers}
}

@inproceedings{dinola23a,
	title = {A pointfree approach to measurability and statistical models},
	author = {Di Nola, Antonio and Lapenta, Serafina and Lenzi, Giacomo},
	pages = {189--199},
	abstract = {In this work we approach the problem of finding the most <em>natural</em> algebraic structure of the set of all possible random variables on a measurable space, inspired by Nelson's point of view. We build our work on previous papers by the same authors and set our investigation in the framework of MV-algebras and algebraic logic. We approach the problem from the perspective of pointfree topology, in order to take the notion of random variable as the primitive one. In the final part of the paper we approach statistical models from the point of view of algebra and category theory, providing a different and perhaps more insightful justification for our logico-algebraic approach to the notion.},
	keywords = {MV-algebras, pointfree topology, probability, measurable functions, statistical models},
	section = {Papers}
}

@inproceedings{dubois23a,
	title = {Eliciting hybrid probability-possibility functions and their decision evaluation models},
	author = {Dubois, Didier and Guillaume, Romain and Rico, Agn\`es},
	pages = {200--209},
	abstract = {We focus on a decision tree model under uncertainty using so-called hybrid probability-possibility functions. They allow to handle behaviours lying between possibilistic decision making and probabilistic decision making while keeping the good properties of both approaches namely <em>Dynamic Consistency</em>, <em>Consequentialism</em> and <em>Tree Reduction</em>. We shed light on the various utility functionals in this setting. More precisely, in this paper, we investigate the question of parameterizing the compromise between possibilistic and probabilisic models in different contexts. To this end, we outline elicitation methods.},
	keywords = {decision under uncertainty, possibility theory, decomposable measures},
	section = {Papers}
}

@inproceedings{erreygers23a,
	title = {Sublinear expectations for countable-state uncertain processes},
	author = {Erreygers, Alexander},
	pages = {210--221},
	abstract = {Sublinear expectations for uncertain processes have received a lot of attention recently, particularly methods to extend a downward-continuous sublinear expectation on the bounded finitary functions to one on the non-finitary functions. In most of the approaches the domain of the extension is not very rich because it is limited to bounded measurable functions on the set of all paths. This contribution alleviates this problem in the countable-state case by extending, under a mild condition, to the extended real measurable functions on the set of càdlàg paths, and investigates when a sublinear Markov semigroup induces a sublinear expectation that satisfies this mild condition.},
	keywords = {convex expectation, (coherent) upper expectation, monotone convergence, sublinear Markov process, sublinear Markov semigroup},
	section = {Papers}
}

@inproceedings{fischer23a,
	title = {A comparison between a frequentist, {B}ayesian and imprecise {B}ayesian approach to delay time maintenance},
	author = {Fischer, Marc},
	pages = {222--229},
	abstract = {Delay time models are stochastic maintenance decision aid tools that divide the failure time of a system into the appearance of a defect and its evolution towards a breakdown. In this study, an imprecise Bayesian approach to delay time modelling has been developed and compared with the frequentist and precise Bayesian approach based on a virtual maintenance problem. The conditional failure rate was the unknown parameter that had to be estimated via eight samples of failure times of increasing size. The goal was to minimise two loss functions related to the downtime and cost. The frequentist and precise Bayesian methods converge towards the optimal decision as the sample size grows but are strongly sub-optimal when no or only few data are available. The imprecise Bayesian approach based on E-admissibility returns large decision intervals in the lack of data, thereby straightforwardly representing the crucial difference between knowledge and ignorance.},
	keywords = {stochastic maintenance, robust Bayesianism, decision rules, frequentism},
	section = {Papers}
}

@inproceedings{frohlich23a,
	title = {Towards a strictly frequentist theory of imprecise probability},
	author = {Fr\"ohlich, Christian and Derr, Rabanus and Williamson, Robert C.},
	pages = {230--240},
	abstract = {Strict frequentism defines probability as the limiting relative frequency in an infinite sequence. What if the limit does not exist? We present a broader theory, which is applicable also to statistical phenomena that exhibit diverging relative frequencies. In doing so, we develop a close connection with imprecise probability: the cluster points of relative frequencies yield a coherent upper prevision. We show that a natural frequentist definition of conditional probability recovers the generalized Bayes rule. We prove constructively that, for a finite set of elementary events, there exists a sequence for which the cluster points of relative frequencies coincide with a prespecified set, thereby providing strictly frequentist semantics for coherent upper previsions.},
	keywords = {imprecise probability, strict frequentism, divergent relative frequencies, von Mises, Ivanenko},
	section = {Papers}
}

@inproceedings{guillaume23a,
	title = {Robust possibilistic production planning under temporal demand uncertainty with knowledge on dependencies},
	author = {Guillaume, Romain},
	pages = {241--248},
	abstract = {In this paper, we deal with production planning problem under temporal demand uncertainty. More precisely, a demand forecasting for a given period could move backward in time or forward in time. We investigate the case where knowledge on dependencies on demand is available. This knowledge is taken into account through a family of copula function. The aim results of the paper are: (a) this approach do not increase the complexity of production planning problem, (b) limit the conservatism of fuzzy robust approach for production planning problem and evaluates more precisely the necessity that the cost of a production plan does not exceed a certain threshold.},
	keywords = {production planning, fuzzy robust optimization, possibility theory},
	section = {Papers}
}

@inproceedings{jann23a,
	title = {Testing the coherence of data and external intervals via an imprecise {S}argan-{H}ansen test},
	author = {Jann, Martin},
	pages = {249--258},
	abstract = {When information about a population is sparse, it is difficult to test whether a data set originated from that population. In applied research, however, researchers often have access to external information in the form of (central) statistical moments such as mean or variance. To compensate for the uncertainty in the external point values, this paper uses external intervals instead to represent the information about moments. The Sargan-Hansen test from the generalized method of moments framework is used, which exploits point-valued external information about moments in the presence of a statistical model to test whether data and external information are in conflict. For the Sargan-Hansen test, a separability result is derived with respect to the model and the external information. This result leads to a simplification of the test in terms of its analytical form and the calculation of the test statistics. To allow the use of external intervals instead of point values, an imprecise version of the Sargan-Hansen test is created using the Gamma-maximin decision rule. Assuming that the variables are normally distributed, a small sample version of this imprecise Sargan-Hansen test is derived. The power and type I errors of the developed tests are analyzed and compared in a simulation study in different small sample scenarios.},
	keywords = {imprecise external information, information-data conflict, generalized method of moments, Sargan-Hansen test, credal set, robustness},
	section = {Papers}
}

@inproceedings{jirousek23a,
	title = {On the relationship between graphical and compositional models for the {D}empster-{S}hafer theory of belief functions},
	author = {Jirou\v{s}ek, Radim and Kratochv{\'\i}l, V\'aclav and Shenoy, Prakash P.},
	pages = {259--269},
	abstract = {This paper studies the relationship between graphical and compositional models representing joint belief functions. In probability theory, the class of Bayesian networks (directed graphical models) is equivalent to compositional models. Such an equivalence does not hold for the Dempster-Shafer belief function theory. We show that each directed graphical belief function model can be represented as a compositional model, but the converse does not hold. As there are two composition operators for belief functions, there are two types of compositional models. In studying their relation to graphical models, they are closely connected. Namely, one is more specific than the other. A precise relationship between these two composition operators is described.},
	keywords = {joint belief functions, conditional independence, Markov models, composition operators, Dempster's combination rule, conditionals},
	section = {Papers}
}

@inproceedings{konek23a,
	title = {Evaluating imprecise forecasts},
	author = {Konek, Jason},
	pages = {270--279},
	abstract = {This paper will introduce a new class of IP scoring rules for sets of almost desirable gambles. A set of almost desirable gambles $\mathcal{D}$ is evaluable for what might be called generalised type 1 and type 2 error. Generalised type 1 error is roughly a matter of the extent to which $\mathcal{D}$ encodes false judgments of desirability. Generalised type 2 error is roughly a matter of the extent to which $\mathcal{D}$ fails to encode true judgments of desirability. IP scoring rules are penalty functions that average these two types of error. To demonstrate the viability of IP scoring rules, we must show that for any coherent $\mathcal{D}$ you might choose, we can construct an IP scoring rule that renders it admissible. Moreover, every other admissible model relative to that scoring rule is also coherent. This paper makes progress toward that goal. We will also compare the class of scoring rules developed here with the results by Seidenfeld, Schervish, and Kadane from 2012,which establish that there is no strictly proper, continuous real-valued scoring rule for lower and upper probability forecasts.},
	keywords = {scoring rules, accuracy, forecasting, lower previsions, closed convex sets of probabilities, sets of almost desirable gambles},
	section = {Papers}
}

@inproceedings{lapenta23a,
	title = {The logic {FP}({\L},{\L}) and two-sorted equational states},
	author = {Lapenta, Serafina and Napolitano, Sebastiano and Spada, Luca},
	pages = {280--287},
	abstract = {The logic {FP(\L,\L)} was introduced by L. Godo and T. Flaminio as an expansion of {\L}ukasiewicz logic with a modality, to reason about the probability of vague events. We prove that {FP(\L,\L)} is complete with respect to a class of two-sorted algebras, called <em>equational states</em>. They are an equational presentation of the well-known theory of states over lattice ordered groups.},
	keywords = {{\L}ukasiewicz logic, state, completeness, two-sorted algebras},
	section = {Papers}
}

@inproceedings{malinowski23a,
	title = {Uncertainty propagation using copulas in a 3{D} stereo matching pipeline},
	author = {Malinowski, Roman and Destercke, S\'ebastien and Dubois, Emmanuel and Dumas, Lo{\"\i}c and Sarrazin, Emmanuelle},
	pages = {288--298},
	abstract = {This contribution presents a concrete example of uncertainty propagation in a stereo matching pipeline. It considers the problem of matching pixels between pairs of images whose radiometry is uncertain and modeled by possibility distributions. Copulas serve as dependency models between variables and are used to propagate the imprecise models. The propagation steps are detailed in the simple case of the Sum of Absolute Difference cost function for didactic purposes. The method results in an imprecise matching cost curve. To reduce computation time, a sufficient condition for conserving possibility distributions after the propagation is also presented. Finally, results are compared with Monte Carlo simulations, indicating that the method produces envelopes capable of correctly estimating the matching cost.},
	keywords = {imprecise probabilities, possibility distribution, copulas, uncertainty propagation, stereo matching},
	section = {Papers}
}

@inproceedings{martin23a,
	title = {Fiducial inference viewed through a possibility-theoretic inferential model lens},
	author = {Martin, Ryan},
	pages = {299--310},
	abstract = {Fisher's fiducial argument is widely viewed as a failed version of Neyman's theory of confidence limits. But Fisher's goal---Bayesian-like probabilistic uncertainty quantification without priors---was more ambitious than Neyman's, and it's not out of reach. I've recently shown that reliable, prior-free probabilistic uncertainty quantification must be grounded in the theory of imprecise probability, and I've put forward a possibility-theoretic solution that achieves it. This has been met with resistance, however, in part due to the statistical community's singular focus on confidence limits. Indeed, if imprecision isn't needed to answer confidence-limit-related questions, then what's the point? In this paper, for a class of practically useful models, I explain specifically why the fiducial argument gives valid confidence limits, i.e., it's the ``best probabilistic approximation'' of the possibilistic solution I recently advanced. This sheds new light on what the fiducial argument is doing and on what's lost in terms of reliability when imprecision is ignored and the fiducial argument is pushed for more than just confidence limits.},
	keywords = {Bayesian, confidence distribution, false confidence, generalized fiducial, group invariance},
	section = {Papers}
}

@inproceedings{martinbordini23a,
	title = {Learning calibrated belief functions from conformal predictions},
	author = {Martin Bordini, Vitor and Destercke, S\'ebastien and Quost, Benjamin},
	pages = {311--320},
	abstract = {We consider the problem of supervised classification. We focus on the problem of calibrating the classifier's outputs. We show that the p-values provided by Inductive Conformal Prediction (ICP) can be interpreted as a possibility distribution over the set of classes. This allows us to use ICP to compute a predictive belief function which is calibrated by construction. We also propose a learning method which provides p-values in a simpler and faster way, by making use of a multi-output regression model. Results obtained on the Cifar10 and Digits data sets show that our approach is comparable to standard ICP in terms of accuracy and calibration, while offering a reduced complexity and avoiding the use of a calibration set.},
	keywords = {conformal prediction, Dempster-Shafer, belief functions, calibration},
	section = {Papers}
}

@inproceedings{maua23a,
	title = {Specifying credal sets with probabilistic answer set programming},
	author = {Mau\'a, Denis Deratani and Cozman, Fabio Gagliardi},
	pages = {321--332},
	abstract = {Probabilistic Answer Set Programming offers an intuitive and powerful declarative language to represent uncertainty about combinatorial structures. Remarkably, under the credal semantics, such programs can specify any infinitely monotone Choquet Capacity in an intuitive way. Yet, one might be interested in specifying more general credal sets. We examine how probabilistic answer set programs can be extended to represent more general credal sets with constructs that allow for imprecise probability values. We characterize the credal sets that can be captured with various languages, and discuss the expressivity and complexity added by the use of imprecision in probabilistic constructs.},
	keywords = {probabilistic logics, answer set programming, credal networks},
	section = {Papers}
}

@inproceedings{miranda23b,
	title = {Vertical barrier models as unified distortions},
	author = {Miranda, Enrique and Pelessoni, Renato and Vicig, Paolo},
	pages = {333--343},
	abstract = {Vertical Barrier Models (VBM) are a family of imprecise probability models that generalise a number of well known distortion/neighbourhood models (such as the Pari-Mutuel Model, the Linear-Vacuous Model, and others) while still being relatively simple. Several of their properties were established by Pelessoni, Vicig, and Corsato. In this paper we explore, in a finite framework, further facets of these models: their interpretation as neighbourhood models, the structure of their credal set in terms of maximum number of its extreme points, the result of merging operations with VBMs, conditions for VBMs to be belief functions or possibility measures.},
	keywords = {vertical barrier models, distortion models, neighbourhood models, 2-monotonicity, belief functions, possibility measures},
	section = {Papers}
}

@inproceedings{miranda23c,
	title = {A study of {J}effrey's rule with imprecise probability models},
	author = {Miranda, Enrique and Van Camp, Arthur},
	pages = {344--355},
	abstract = {Jeffrey's rule tells us how to update our beliefs about a probability measure when we have updated information conditional on some partition of the possibility space, while keeping the original marginal information on this partition. It is linked to the law of total probability, and is therefore connected to the notion of marginal extension of coherent lower previsions. In this paper, we investigate its formulation for some other imprecise probability models that are either more general (choice functions) or more particular (possibility measures, distortion models) than coherent lower previsions.},
	keywords = {Jeffrey's rule, marginal extension, coherent lower previsions, sets of desirable gambles, non-additive measures, choice functions},
	section = {Papers}
}

@inproceedings{molinari23a,
	title = {Trust the evidence: two deference principles for imprecise probabilities},
	author = {Molinari, Giacomo},
	pages = {356--366},
	abstract = {Our intuition that rational agents should value the evidence can be captured by a well-known theorem due to I. J. Good. However, Good's theorem fails when agents have imprecise credences, raising the worry that agents with imprecise credences don't value the evidence. This essay shows a different way to capture our starting intuition, as the claim that rational agents defer to their informed selves. I introduce and motivate two deference principles for imprecise probabilities, and show that rational imprecise agents defer to their informed selves according to these principles. This shows a sense in which imprecise agents value the evidence. I end by comparing the deference principles introduced here with an alternative from the literature.},
	keywords = {imprecise probability, deference principles, reflection, Good's theorem},
	section = {Papers}
}

@inproceedings{montes23a,
	title = {Neighbourhood models induced by the {E}uclidean distance and the {K}ullback-{L}eibler divergence},
	author = {Montes, Ignacio},
	pages = {367--378},
	abstract = {Neighbourhood or distortion models are particular imprecise probability models that appear by creating a neighbourhood around a probability measure given a distorting function and a distortion parameter. This paper investigates the distortion models obtained when considering the Euclidean distance or the Kullback-Leibler divergence as distorting function. We analyse the main properties of the credal sets induced by these two distorting functions as well as the main properties of the associated coherent lower previsions. To conclude the paper, we compare these two models with other well-known distortion models: the pari-mutuel, linear vacuous, constant odds ratio and total variation models.},
	keywords = {Euclidean distance, Kullback-Leibler divergence, distortion model, lower prevision, credal set},
	section = {Papers}
}

@inproceedings{paul23a,
	title = {Expected time averages in {M}arkovian imprecise jump processes: a graph-theoretic characterisation of weak ergodicity},
	author = {Paul, Yema and Erreygers, Alexander and De Bock, Jasper},
	pages = {379--389},
	abstract = {Markovian imprecise jump processes provide a way to express model uncertainty about Markovian jump processes. The dynamics are not governed by a unique rate matrix, but are instead partially specified by a set of such matrices. Since the dynamics are partially specified, the resulting expected time averages are no longer uniquely determined either, and one then resorts to tight lower and upper bounds on them. In this paper, we are interested in the existence of an asymptotic limit of these upper and lower bounds, as the time horizon becomes infinite. When those limits exist and are furthermore independent of the choice of the process's initial state, we say that the process is weakly ergodic. Our main contribution is a necessary and sufficient condition for a Markovian imprecise jump process to be weakly ergodic, expressed in terms of simple graph-theoretic conditions on its set of rate matrices.},
	keywords = {Markovian jump process, imprecise probabilities, expected time averages, weak ergodicity, upper rate operator},
	section = {Papers}
}

@inproceedings{persiau23a,
	title = {Imprecision in martingale-theoretic prequential randomness},
	author = {Persiau, Floris and de Cooman, Gert},
	pages = {390--400},
	abstract = {In a prequential approach to algorithmic randomness, probabilities for the next outcome can be forecast `on the fly' without the need for fully specifying a probability measure on all possible sequences of outcomes, as is the case in the more standard approach. We take the first steps in allowing for probability intervals instead of precise probabilities in this prequential approach, based on ideas from our earlier imprecise-probabilistic and martingale-theoretic account of algorithmic randomness. We define what it means for an infinite sequence $(I_1,x_1,I_2,x_2,\dots)$ of successive interval forecasts $I_k$ and subsequent binary outcomes $x_k$ to be random. We compare the resulting prequential randomness notion with the more standard one, and investigate where both randomness notions coincide, as well as where their properties correspond.},
	keywords = {superfarthingales, algorithmic randomness, prequential probability forecasting, imprecise probabilities, computability, probability intervals},
	section = {Papers}
}

@inproceedings{petturiti23a,
	title = {No-arbitrage pricing with $\alpha$-{DS} mixtures in a market with bid-ask spreads},
	author = {Petturiti, Davide and Vantaggi, Barbara},
	pages = {401--411},
	abstract = {This paper introduces $\alpha$-DS mixtures, which are normalized capacities that can be represented (generally not in a unique way) as the $\alpha$-mixture of a belief function and its dual plausibility function. Assuming a finite state space, such capacities extend to a Choquet expectation functional that can be given a Hurwicz-like expression. In turn, $\alpha$-DS mixtures and their Choquet expectations appear to be particularly suitable to model prices in a market with frictions, where bid-ask prices are usually averaged taking $\alpha = \frac{1}{2}$. For this, we formulate a no-arbitrage one-period pricing problem in the framework of $\alpha$-DS mixtures and prove the analogues of the first and second fundamental theorems of asset pricing. Finally, we perform a calibration on market data to derive a market consistent no-arbitrage $\alpha$-DS mixture pricing rule.},
	keywords = {$\alpha$-DS mixture, no-arbitrage pricing, bid-ask spreads},
	section = {Papers}
}

@inproceedings{rodemann23a,
	title = {In all likelihoods: robust selection of pseudo-labeled data},
	author = {Rodemann, Julian and Jansen, Christoph and Schollmeyer, Georg and Augustin, Thomas},
	pages = {412--425},
	abstract = {Self-training is a simple yet effective method within semi-supervised learning. Self-training's rationale is to iteratively enhance training data by adding pseudo-labeled data. Its generalization performance heavily depends on the selection of these pseudo-labeled data (PLS). In this paper, we render PLS more robust towards the involved modeling assumptions. To this end, we treat PLS as a decision problem, which allows us to introduce a generalized utility function. The idea is to select pseudo-labeled data that maximize a multi-objective utility function. We demonstrate that the latter can be constructed to account for different sources of uncertainty and explore three examples: model selection, accumulation of errors and covariate shift. In the absence of second-order information on such uncertainties, we furthermore consider the generic approach of the generalized Bayesian $\alpha$-cut updating rule for credal sets. We spotlight the application of three of our robust extensions on both simulated and three real-world data sets. In a benchmarking study, we compare these extensions to traditional PLS methods. Results suggest that robustness with regard to model choice can lead to substantial accuracy gains.},
	keywords = {semi-supervised learning, self-training, pseudo labeling, generalized Bayes, model selection, covariate shift, generalized updating rules},
	section = {Papers}
}

@inproceedings{shenoy23a,
	title = {On distinct belief functions in the {D}empster-{S}hafer theory},
	author = {Shenoy, Prakash P.},
	pages = {426--437},
	abstract = {Dempster's combination rule is the centerpiece of the Dempster-Shafer (D-S) theory of belief functions. In practice, Dempster's combination rule should only be applied to combine two distinct belief functions (in the belief function literature, distinct belief functions are also called independent belief functions). So, the question arises: what constitutes distinct belief functions? We have an answer in Dempster's multi-valued functions semantics for distinct belief functions. The probability functions on the two spaces associated with the multi-valued functions should be independent. In practice, however, we don't always associate a multi-valued function with belief functions in a model. In this article, we discuss the notion of distinct belief functions in graphical models, both directed and undirected. The idea of distinct belief functions corresponds to no double-counting of non-idempotent knowledge semantics of conditional independence. Although we discuss the notion of distinct belief functions in the context of the DS theory, the discussion is valid more broadly to many uncertainty calculi, including probability theory, possibility theory, and Spohn's epistemic belief theory.},
	keywords = {distinct belief functions, Dempster-Shafer belief function theory, belief-function directed graphical model, belief-function undirected graphical model},
	section = {Papers}
}

@inproceedings{skau23a,
	title = {Open world {D}empster-{S}hafer using complementary sets},
	author = {Skau, Erik and Armstrong, Cassandra and Truong, Duc P. and Gerts, David and Sentz, Kari},
	pages = {438--449},
	abstract = {Dempster-Shafer Theory (DST) is a mathematical framework to handle imprecision and uncertainty in reasoning and decision making. One assumption of DST is that of a closed-world, or the assumption that all propositions are known <em>a priori</em>. In this work, we explore an alternative formulation of Dempster-Shafer that allows for the dynamic inclusion of new propositions. Specifically, we expand the framework to include the complement of every set of propositions. This adjustment enables an open-world interpretation that can support unspecified and dynamic propositions as we learn about the problem space. Including complementary sets distinguishes this from previous work in DST where the open world is attributed to the empty set. We demonstrate our open world Dempster-Shafer Theory on a variety of synthetic and real datasets.},
	section = {Papers}
}

@inproceedings{troffaes23a,
	title = {A nonstandard approach to stochastic processes under probability bounding},
	author = {Troffaes, Matthias C. M.},
	pages = {450--460},
	abstract = {This paper studies stochastic processes under probability bounding, using nonstandard conditional lower previsions within the framework of internal set theory. Following Nelson's approach to stochastic processes, we introduce elementary processes which are defined over a finite number of time points and that serve to approximate any standard process, including processes over continuous time. We show that every standard process can be represented by an elementary process, and that the shadow of every elementary process constitutes again a standard process. We then move to demonstrate how elementary processes can be used to define imprecise Markov chains both in discrete and continuous time. To demonstrate the benefits and downsides of this approach, we show how to recover some basic results for continuous time Markov chains through analysis of a nonstandard elementary process.},
	keywords = {imprecise Markov chain, stochastic process, lower prevision, internal set theory, nonstandard analysis},
	section = {Papers}
}

@inproceedings{willot23a,
	title = {Prime implicants as a versatile tool to explain robust classification},
	author = {Willot, H\'eno{\"\i}k and Destercke, S\'ebastien and Belahc\`ene, Khaled},
	pages = {461--471},
	abstract = {In this paper, we investigate how robust classification results can be explained by the notion of prime implicants, focusing on explaining pairwise dominance relations. By robust, we mean that we consider imprecise models that may abstain to classify or to compare two classes when information is insufficient. This will be reflected by considering (convex) sets of probabilities. By prime implicants, we understand a subset of attributes, minimal w.r.t. inclusion, that we need to know or specify before reaching a specified conclusion (either of dominance or non-dominance between two classes). After presenting the general concepts, we derive them in the case of the well-known naive credal classifier.},
	keywords = {robust classifier, explainability, prime implicants, imprecise probabilities, naive credal classifier},
	section = {Papers}
}
